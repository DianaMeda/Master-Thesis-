{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "from timm import create_model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import medmnist\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.linalg import inv\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "\n",
    "import timm\n",
    "from timm import create_model\n",
    "\n",
    "from torch_geometric.nn import LabelPropagation\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.linalg import inv\n",
    "import seaborn as sns\n",
    "import medmnist\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import community\n",
    "import community.community_louvain as community\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99de34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dime/Desktop/Thesis/dermamnist_224'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "num_classes = 7\n",
    "num_epochs = 100\n",
    "img_size=224\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccf208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_log):\n",
    "    epochs = range(1, len(metrics_log[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, metrics_log[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, metrics_log[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, metrics_log[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, metrics_log[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, metrics_log[\"train_f1\"], label=\"Train F1 Score\")\n",
    "    plt.plot(epochs, metrics_log[\"val_f1\"], label=\"Validation F1 Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution(class_counts, title):\n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(classes, counts, color='skyblue')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93709bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images_path: str, labels_path: str, transform=None):\n",
    "        self.images = np.load(images_path)\n",
    "        self.labels = np.load(labels_path).flatten()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img] * 3, axis=0)\n",
    "        else:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        lbl = int(self.labels[idx])\n",
    "        return img, lbl\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def load_datasets(img_size=img_size, batch_size=batch_size, shuffle_train=True):\n",
    "    train_dataset = NumpyDataset(\n",
    "        os.path.join(path, 'train_images.npy'),\n",
    "        os.path.join(path, 'train_labels.npy'),\n",
    "        train_transform\n",
    "    )\n",
    "    val_dataset = NumpyDataset(\n",
    "        os.path.join(path, 'val_images.npy'),\n",
    "        os.path.join(path, 'val_labels.npy'),\n",
    "        val_test_transform\n",
    "    )\n",
    "    test_dataset = NumpyDataset(\n",
    "        os.path.join(path, 'test_images.npy'),\n",
    "        os.path.join(path, 'test_labels.npy'),\n",
    "        val_test_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_class_counts = compute_class_counts(os.path.join(path, 'train_labels.npy'))\n",
    "val_class_counts = compute_class_counts(os.path.join(path, 'val_labels.npy'))\n",
    "test_class_counts = compute_class_counts(os.path.join(path, 'test_labels.npy'))\n",
    "\n",
    "print('Train class counts:', train_class_counts)\n",
    "print('Val class counts:', val_class_counts)\n",
    "print('Test class counts:', test_class_counts)\n",
    "\n",
    "train_class_counts = compute_class_counts(os.path.join(path, 'train_labels.npy'))\n",
    "total_train = sum(train_class_counts.values())\n",
    "#  higher weight for less frequent classes\n",
    "class_weights = [total_train / train_class_counts[i] for i in range(num_classes)]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "plot_class_distribution(train_class_counts, 'Train Class Distribution')\n",
    "plot_class_distribution(val_class_counts, 'Validation Class Distribution')\n",
    "plot_class_distribution(test_class_counts, 'Test Class Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227199e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = timm.create_model('vgg16', pretrained=True, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, optimizer, scheduler, scaler, \n",
    "                num_epochs=num_epochs, accumulation_steps=4, patience=5):\n",
    "    metrics_log = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "        \"val_f1\": []\n",
    "    }\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_preds = [], []\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        for step, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        if (step + 1) % accumulation_steps != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_labels, val_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        metrics_log[\"train_loss\"].append(train_loss)\n",
    "        metrics_log[\"train_accuracy\"].append(train_accuracy)\n",
    "        metrics_log[\"train_f1\"].append(train_f1)\n",
    "        metrics_log[\"val_loss\"].append(val_loss)\n",
    "        metrics_log[\"val_accuracy\"].append(val_accuracy)\n",
    "        metrics_log[\"val_f1\"].append(val_f1)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"vgg_derma_best.pth\")\n",
    "            print(\"Validation loss improved. Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Final Train Loss:     {metrics_log['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Train Accuracy: {metrics_log['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final Train F1:       {metrics_log['train_f1'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss:       {metrics_log['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Accuracy:   {metrics_log['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final Val F1:         {metrics_log['val_f1'][-1]:.4f}\")\n",
    "    \n",
    "    plot_metrics(metrics_log)\n",
    "    return metrics_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(seeds=[42, 43, 45], num_epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size, img_size=img_size):\n",
    "    results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n\\n======================= SEED {seed} =======================\")\n",
    "        set_seed(seed)\n",
    "        start_time = time.time()\n",
    "        train_loader, val_loader, test_loader = load_datasets(img_size, batch_size)\n",
    "        \n",
    "        model = timm.create_model('vgg16', pretrained=True, num_classes=num_classes)\n",
    "        model = model.to(device)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        metrics_log = train_model(model, train_loader, val_loader, device,\n",
    "                                  optimizer, scheduler, scaler,\n",
    "                                  num_epochs=num_epochs)\n",
    "        \n",
    "        final_train_acc = metrics_log[\"train_accuracy\"][-1]\n",
    "        final_train_f1  = metrics_log[\"train_f1\"][-1]\n",
    "        final_val_acc   = metrics_log[\"val_accuracy\"][-1]\n",
    "        final_val_f1    = metrics_log[\"val_f1\"][-1]\n",
    "        \n",
    "        results.append((final_train_acc, final_train_f1, final_val_acc, final_val_f1))\n",
    "    \n",
    "    results = np.array(results)\n",
    "    means = results.mean(axis=0)\n",
    "    stds  = results.std(axis=0)\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n======================= SUMMARY =======================\")\n",
    "    print(f\"Seeds used: {seeds}\")\n",
    "    print(f\"Train Accuracy: mean={means[0]:.4f}, std={stds[0]:.4f}\")\n",
    "    print(f\"Train F1:       mean={means[1]:.4f}, std={stds[1]:.4f}\")\n",
    "    print(f\"Val Accuracy:   mean={means[2]:.4f}, std={stds[2]:.4f}\")\n",
    "    print(f\"Val F1:         mean={means[3]:.4f}, std={stds[3]:.4f}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "    return (means, stds)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
