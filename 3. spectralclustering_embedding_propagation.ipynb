{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, eye\n",
    "from scipy.sparse.linalg import inv\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61feab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH        = '/home/dime/Desktop/Thesis/pneumoniamnist_224'\n",
    "FEAT_TRAIN_FILE  = 'vgg_pneumonia_train_features.pth'\n",
    "FEAT_VAL_FILE    = 'vgg_pneumonia_val_features.pth'\n",
    "FEAT_TEST_FILE   = 'vgg_pneumonia_test_features.pth'\n",
    "\n",
    "\n",
    "LABEL_TRAIN_FILE = 'train_labels.npy'\n",
    "VAL_LABEL_FILE   = 'val_labels.npy'\n",
    "TEST_LABEL_FILE  = 'test_labels.npy'\n",
    "\n",
    "CORESET_PREF     = 'coreset_indices_pneumonia_vgg_'\n",
    "\n",
    "K                = 10\n",
    "PROP_ALPHA       = 0.5\n",
    "PERCENTAGES      = [0.1, 1, 10]\n",
    "IMG_SIZE         = 224\n",
    "BATCH            = 16\n",
    "EPOCHS           = 30\n",
    "LR               = 1e-4\n",
    "LAPLACE_LAMBDA   = 1e-3\n",
    "EPS              = 1e-6\n",
    "PATIENCE         = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images = np.load(images_path)\n",
    "        self.labels = np.load(labels_path).flatten()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.images[i]\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img] * 3, axis=-1)\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, int(self.labels[i])\n",
    "\n",
    "class CoresetDataset(Dataset):\n",
    "    def __init__(self, base_ds, indices, soft_labels):\n",
    "        self.base = base_ds\n",
    "        self.indices = np.array(indices, dtype=int)\n",
    "        self.y_soft = torch.from_numpy(soft_labels[self.indices]).float()\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, i):\n",
    "        img, _ = self.base[self.indices[i]]\n",
    "        return img, self.y_soft[i], self.indices[i]\n",
    "\n",
    "class SoftDataset(Dataset):\n",
    "    def __init__(self, base_ds, true_labels, num_classes):\n",
    "        self.base = base_ds\n",
    "        Y = np.eye(num_classes, dtype=np.float32)[true_labels]\n",
    "        self.Y = torch.from_numpy(Y)\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        img, _ = self.base[i]\n",
    "        return img, self.Y[i], i\n",
    "    \n",
    "train_labels = np.load(os.path.join(DATA_PATH, 'train_labels.npy')).flatten()\n",
    "val_labels   = np.load(os.path.join(DATA_PATH, VAL_LABEL_FILE)).flatten()\n",
    "test_labels  = np.load(os.path.join(DATA_PATH, TEST_LABEL_FILE)).flatten()\n",
    "\n",
    "torch.save(torch.from_numpy(train_labels), LABEL_TRAIN_FILE)\n",
    "\n",
    "N_tr, N_va, N_te = len(train_labels), len(val_labels), len(test_labels)\n",
    "C = int(train_labels.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_tr = torch.load(os.path.join(DATA_PATH, FEAT_TRAIN_FILE),\n",
    "                      map_location='cpu').numpy()\n",
    "feats_va = torch.load(os.path.join(DATA_PATH, FEAT_VAL_FILE),\n",
    "                      map_location='cpu').numpy()\n",
    "feats_te = torch.load(os.path.join(DATA_PATH, FEAT_TEST_FILE),\n",
    "                      map_location='cpu').numpy()\n",
    "\n",
    "feats_all = np.vstack([feats_tr, feats_va, feats_te])\n",
    "feats_norm_all = feats_all / (np.linalg.norm(feats_all, axis=1, keepdims=True) + EPS)\n",
    "labels_all = np.concatenate([train_labels, val_labels, test_labels])\n",
    "N_all, d = feats_norm_all.shape\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=K+1).fit(feats_norm_all)\n",
    "dists, _ = nbrs.kneighbors(feats_norm_all)\n",
    "SIGMA2 = np.median(dists[:, -1] ** 2)\n",
    "print(f\"[DEBUG] SIGMAÂ² = {SIGMA2:.3e}\")\n",
    "\n",
    "def build_knn_graph(Z, labels, K, sigma):\n",
    "    N = Z.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=K+1).fit(Z)\n",
    "    dists, inds = nbrs.kneighbors(Z)\n",
    "    rows, cols, vals = [], [], []\n",
    "    for i in range(N):\n",
    "        for j in inds[i, 1:]:\n",
    "            w = math.exp(-np.linalg.norm(Z[i] - Z[j])**2 / sigma**2)\n",
    "            if labels[i] != labels[j]:\n",
    "                w *= 0.5\n",
    "            rows.append(i); cols.append(j); vals.append(w)\n",
    "    W = csr_matrix((vals + vals, (rows + cols, cols + rows)), shape=(N, N))\n",
    "    W = W + EPS * eye(N, N)\n",
    "    print(f\"[DEBUG] W nnz={W.nnz}, density={W.nnz/(N*N):.2e}\")\n",
    "    return W\n",
    "\n",
    "A0_all = build_knn_graph(feats_norm_all, labels_all, K, math.sqrt(SIGMA2))\n",
    "D0 = np.array(A0_all.sum(1)).ravel()\n",
    "Dinv2_0 = csr_matrix((1.0/np.sqrt(D0+EPS), (np.arange(N_all), np.arange(N_all))),\n",
    "                     shape=(N_all, N_all))\n",
    "L_norm_all = eye(N_all, N_all) - Dinv2_0.dot(A0_all.dot(Dinv2_0))\n",
    "\n",
    "P_all = inv(eye(N_all, N_all) - PROP_ALPHA * L_norm_all)\n",
    "Z_prop_all = P_all.dot(feats_norm_all)\n",
    "Z_prop_norm_all = Z_prop_all / (np.linalg.norm(Z_prop_all, axis=1, keepdims=True) + EPS)\n",
    "\n",
    "W_prop = build_knn_graph(Z_prop_norm_all, labels_all, K, math.sqrt(SIGMA2))\n",
    "deg_prop = np.array(W_prop.sum(1)).ravel()\n",
    "\n",
    "feats_norm_tr = feats_norm_all[:N_tr]\n",
    "feats_norm_va = feats_norm_all[N_tr:N_tr+N_va]\n",
    "feats_norm_te = feats_norm_all[N_tr+N_va:]\n",
    "\n",
    "Z_prop_tr     = Z_prop_norm_all[:N_tr]\n",
    "Z_prop_va     = Z_prop_norm_all[N_tr:N_tr+N_va]\n",
    "Z_prop_te     = Z_prop_norm_all[N_tr+N_va:]\n",
    "\n",
    "\n",
    "def compute_label_propagation(W, labels, C, iters=2):\n",
    "    N = W.shape[0]\n",
    "    Dinv = csr_matrix((1.0/(np.array(W.sum(1)).ravel()+EPS),\n",
    "                       (np.arange(N), np.arange(N))),\n",
    "                      shape=(N, N))\n",
    "    Y = np.eye(C, dtype=np.float32)[labels]\n",
    "    for _ in range(iters):\n",
    "        Y = Dinv.dot(W.dot(Y))\n",
    "    ent = -np.sum(Y * np.log(Y + 1e-12), axis=1).mean()\n",
    "    print(f\"[DEBUG] Y_smooth entropy: {ent:.3f}\")\n",
    "    return Y\n",
    "\n",
    "Y_smooth_all = compute_label_propagation(W_prop, labels_all, C)\n",
    "Y_smooth_tr  = Y_smooth_all[:N_tr]\n",
    "\n",
    "# class weights\n",
    "cnts = Counter(train_labels)\n",
    "tot  = sum(cnts.values())\n",
    "class_weights = torch.tensor([tot/cnts[c] for c in range(C)],\n",
    "                             device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be264f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_graph_reg(model, train_loader, val_loader,\n",
    "                         Wg, degg, lam, cls_w, device,\n",
    "                         optimizer, scheduler, scaler,\n",
    "                         num_epochs, patience, checkpoint_path):\n",
    "    best_val, pat_cnt = float('inf'), 0\n",
    "    for ep in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        for imgs, y_soft, idxs in train_loader:\n",
    "            imgs, y_soft = imgs.to(device), y_soft.to(device)\n",
    "            idxs_np = idxs.numpy()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=bool(scaler)):\n",
    "                out = model(imgs)\n",
    "\n",
    "                h = -(y_soft * F.log_softmax(out, dim=1)).sum(1)\n",
    "                sw = cls_w[y_soft.argmax(1)]\n",
    "                loss_ce = (h * sw).mean()\n",
    "\n",
    "                bd = torch.sqrt(torch.from_numpy(degg[idxs_np]+EPS).to(device)).unsqueeze(1)\n",
    "                outn = out / bd\n",
    "                Wb   = torch.from_numpy(Wg[idxs_np][:, idxs_np].toarray()).to(device)\n",
    "                sqd  = (outn.unsqueeze(1) - outn.unsqueeze(0)).pow(2).sum(-1)\n",
    "                loss_reg = 0.5 * (Wb * sqd).sum()\n",
    "                loss = loss_ce + lam * loss_reg\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, corr, tot = 0., 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, yv, _ in val_loader:\n",
    "                imgs, yv = imgs.to(device), yv.to(device)\n",
    "                out = model(imgs)\n",
    "                val_loss += -(yv * F.log_softmax(out,1)).sum(1).mean().item()\n",
    "                preds = out.argmax(1)\n",
    "                corr += (preds == yv.argmax(1)).sum().item()\n",
    "                tot  += preds.size(0)\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        val_acc = corr / tot\n",
    "        if val_loss < best_val:\n",
    "            best_val, pat_cnt = val_loss, 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            pat_cnt += 1\n",
    "            if pat_cnt >= patience:\n",
    "                break\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_val  = os.path.join(DATA_PATH, 'derma_vgg_3.csv')\n",
    "res_test = os.path.join(DATA_PATH, 'derma_vgg_3.csv')\n",
    "for pth, hdr in [(res_val,  ['percentage','mean_f1_macro','std_f1_macro',\n",
    "                             'mean_train_time_s','std_train_time_s',\n",
    "                             'mean_val_acc','std_val_acc',\n",
    "                             'mean_hausdorff','std_hausdorff']),\n",
    "                 (res_test,['percentage','mean_f1_macro','std_f1_macro',\n",
    "                             'mean_train_time_s','std_train_time_s',\n",
    "                             'mean_test_acc','std_test_acc',\n",
    "                             'mean_hausdorff','std_hausdorff'])]:\n",
    "    if os.path.exists(pth):\n",
    "        os.remove(pth)\n",
    "    with open(pth, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow(hdr)\n",
    "\n",
    "val_ds    = NumpyDataset(os.path.join(DATA_PATH, 'val_images.npy'),\n",
    "                         os.path.join(DATA_PATH, VAL_LABEL_FILE),\n",
    "                         transform=val_transform)\n",
    "test_ds   = NumpyDataset(os.path.join(DATA_PATH, 'test_images.npy'),\n",
    "                         os.path.join(DATA_PATH, TEST_LABEL_FILE),\n",
    "                         transform=val_transform)\n",
    "\n",
    "val_loader  = DataLoader(SoftDataset(val_ds,   val_labels, C),\n",
    "                         batch_size=BATCH, shuffle=False)\n",
    "test_loader = DataLoader(SoftDataset(test_ds,  test_labels, C),\n",
    "                         batch_size=BATCH, shuffle=False)\n",
    "\n",
    "for p in PERCENTAGES:\n",
    "    lam      = LAPLACE_LAMBDA * min(1.0, p/5.0)\n",
    "    lr_s     = LR * min(1.0, p/5.0)\n",
    "\n",
    "    metrics_v, metrics_t = [], []\n",
    "    times, hd_vals, hd_tests = [], [], []\n",
    "\n",
    "    for seed in [42, 43, 45]:\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if device.type == 'cuda': torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        # coreset\n",
    "        sel = np.load(os.path.join(DATA_PATH, f\"{CORESET_PREF}{int(p)}.npy\"))\n",
    "        train_ds = NumpyDataset(\n",
    "            os.path.join(DATA_PATH, 'train_images.npy'),\n",
    "            os.path.join(DATA_PATH, 'train_labels.npy'),\n",
    "            transform=train_transform\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            CoresetDataset(train_ds, sel, Y_smooth_tr),\n",
    "            batch_size=BATCH, shuffle=True\n",
    "        )\n",
    "\n",
    "        model = create_model('vgg16',\n",
    "                             pretrained=True, num_classes=C).to(device)\n",
    "        opt   = optim.Adam(model.parameters(), lr=lr_s)\n",
    "        sch   = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=PATIENCE)\n",
    "        scaler= torch.cuda.amp.GradScaler() if device.type=='cuda' else None\n",
    "\n",
    "        ckpt_path = os.path.join(DATA_PATH, f'best_p{p}_s{seed}.pth')\n",
    "        t0 = time.time()\n",
    "        best_ckpt = train_with_graph_reg(\n",
    "            model, train_loader, val_loader,\n",
    "            W_prop, deg_prop,\n",
    "            lam, class_weights,\n",
    "            device, opt, sch, scaler,\n",
    "            EPOCHS, PATIENCE, ckpt_path\n",
    "        )\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "        model.load_state_dict(torch.load(best_ckpt))\n",
    "        preds_v, trues_v, idxs_v = [], [], []\n",
    "        for imgs, yv, idxs in val_loader:\n",
    "            out = model(imgs.to(device))\n",
    "            preds_v.extend(out.argmax(1).cpu().tolist())\n",
    "            trues_v.extend(yv.argmax(1).cpu().tolist())\n",
    "            idxs_v.extend(idxs.tolist())\n",
    "        metrics_v.append((f1_score(trues_v,preds_v,average='macro'),\n",
    "                          accuracy_score(trues_v,preds_v)))\n",
    "\n",
    "        X = feats_norm_va[idxs_v]\n",
    "        Y = Z_prop_va[idxs_v]\n",
    "        h_ab = directed_hausdorff(X, Y)[0]\n",
    "        h_ba = directed_hausdorff(Y, X)[0]\n",
    "        hd_vals.append(max(h_ab, h_ba))\n",
    "\n",
    "        preds_t, trues_t, idxs_t = [], [], []\n",
    "        for imgs, yv, idxs in test_loader:\n",
    "            out = model(imgs.to(device))\n",
    "            preds_t.extend(out.argmax(1).cpu().tolist())\n",
    "            trues_t.extend(yv.argmax(1).cpu().tolist())\n",
    "            idxs_t.extend(idxs.tolist())\n",
    "        metrics_t.append((f1_score(trues_t,preds_t,average='macro'),\n",
    "                          accuracy_score(trues_t,preds_t)))\n",
    "\n",
    "        X = feats_norm_te[idxs_t]\n",
    "        Y = Z_prop_te[idxs_t]\n",
    "        h_ab = directed_hausdorff(X, Y)[0]\n",
    "        h_ba = directed_hausdorff(Y, X)[0]\n",
    "        hd_tests.append(max(h_ab, h_ba))\n",
    "\n",
    "    arr_v = np.array(metrics_v)\n",
    "    arr_t = np.array(metrics_t)\n",
    "\n",
    "    row_v = [p,\n",
    "             arr_v[:,0].mean(), arr_v[:,0].std(),\n",
    "             np.mean(times),      np.std(times),\n",
    "             arr_v[:,1].mean(),   arr_v[:,1].std(),\n",
    "             np.mean(hd_vals),    np.std(hd_vals)]\n",
    "    row_t = [p,\n",
    "             arr_t[:,0].mean(), arr_t[:,0].std(),\n",
    "             np.mean(times),      np.std(times),\n",
    "             arr_t[:,1].mean(),   arr_t[:,1].std(),\n",
    "             np.mean(hd_tests),   np.std(hd_tests)]\n",
    "\n",
    "    with open(res_val,  'a', newline='') as f:\n",
    "        csv.writer(f).writerow(row_v)\n",
    "    with open(res_test, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow(row_t)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
