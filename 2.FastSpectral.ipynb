{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np, torch, scipy.sparse as sp\n",
    "from collections import Counter\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, adjusted_rand_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import faiss\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images = np.load(images_path)\n",
    "        self.labels = np.load(labels_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = (self.images[idx] * 255).astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img]*3, axis=-1)\n",
    "        img = torch.from_numpy(img.transpose(2,0,1)).float() / 255.0\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = int(self.labels[idx])\n",
    "        return img, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0), antialias=True),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224), antialias=True),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn_graph(features, k_nn=10):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_nn+1, algorithm='auto').fit(features)\n",
    "    distances, indices = nbrs.kneighbors(features)\n",
    "    sigmas = distances[:, -1]\n",
    "    G = nx.Graph()\n",
    "    N = features.shape[0]\n",
    "    G.add_nodes_from(range(N))\n",
    "    for i in range(N):\n",
    "        for j_idx, j in enumerate(indices[i,1:], start=1):\n",
    "            if sigmas[i]>0 and sigmas[j]>0:\n",
    "                w = math.exp(-(distances[i,j_idx]**2)/(sigmas[i]*sigmas[j]))\n",
    "            else:\n",
    "                w = 0.0\n",
    "            G.add_edge(i,j,weight=float(w))\n",
    "    return G\n",
    "\n",
    "from coreset_sc import CoresetSpectralClustering\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def convert_from_csr_matrix(matrix: csr_matrix):\n",
    "    matrix.sort_indices()\n",
    "    indices = matrix.indices.astype(np.uintp)  \n",
    "    indptr = matrix.indptr.astype(np.uintp)  \n",
    "    if matrix.data.dtype == np.float32:\n",
    "        data = matrix.data.astype(np.float64)\n",
    "    elif matrix.data.dtype == np.float64:\n",
    "        data = matrix.data\n",
    "    else:\n",
    "        raise ValueError(\"Data type not supported, expected float32 or float64.\")\n",
    "\n",
    "    return data, indices, indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd226a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_subset(model, train_loader, val_loader, device,\n",
    "                       optimizer, scheduler, scaler, num_epochs=20,\n",
    "                       accumulation_steps=4, patience=5, class_weights=None):\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    logs = {k:[] for k in ['train_loss','train_acc','train_f1','val_loss','val_acc','val_f1']}\n",
    "    model.to(device)\n",
    "    best_val = float('inf'); wait=0\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(); running=0; all_y=[]; all_p=[]\n",
    "        optimizer.zero_grad()\n",
    "        for i,(x,y) in enumerate(tqdm(train_loader, desc=f\"Train {epoch+1}/{num_epochs}\")):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x); loss = criterion(out,y)/accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1)%accumulation_steps==0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
    "            running += loss.item()*accumulation_steps\n",
    "            preds = out.argmax(1)\n",
    "            all_y.extend(y.cpu().numpy()); all_p.extend(preds.cpu().numpy())\n",
    "        train_loss = running/len(train_loader)\n",
    "        train_acc = accuracy_score(all_y, all_p)\n",
    "        train_f1 = f1_score(all_y, all_p, average='weighted')\n",
    "\n",
    "        model.eval(); vl=0; vy=[]; vp=[]\n",
    "        with torch.no_grad():\n",
    "            for x,y in tqdm(val_loader, desc=\"Validate\"):  \n",
    "                x,y = x.to(device), y.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out = model(x); loss = criterion(out,y)\n",
    "                vl += loss.item()\n",
    "                prs = out.argmax(1)\n",
    "                vy.extend(y.cpu().numpy()); vp.extend(prs.cpu().numpy())\n",
    "        val_loss = vl/len(val_loader)\n",
    "        val_acc = accuracy_score(vy,vp)\n",
    "        val_f1 = f1_score(vy,vp,average='weighted')\n",
    "        logs['train_loss'].append(train_loss); logs['train_acc'].append(train_acc); logs['train_f1'].append(train_f1)\n",
    "        logs['val_loss'].append(val_loss); logs['val_acc'].append(val_acc); logs['val_f1'].append(val_f1)\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val:\n",
    "            best_val=val_loss; wait=0\n",
    "        else:\n",
    "            wait+=1;  \n",
    "            if wait>=patience:\n",
    "                break\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_mink(\n",
    "    features, labels, val_dataset,\n",
    "    percentages=[0.1,1,5,10], seeds=[42,43,45],\n",
    "    k_nn=50,          \n",
    "    batch_size=16, lr=1e-4, device='cpu'\n",
    "):\n",
    "    MIN_KNN = 5     \n",
    "\n",
    "    results = {}\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    total  = len(labels)\n",
    "    cw     = torch.tensor([total/counts[c] for c in sorted(counts)],\n",
    "                         device=device, dtype=torch.float32)\n",
    "\n",
    "    for p in percentages:\n",
    "        k_curr = max(MIN_KNN, int(k_nn * (1 - p/100.0)))\n",
    "        print(f\"\\n— ε={p}%: using k_nn={k_curr}\")\n",
    "\n",
    "        stats=[]\n",
    "        for seed in seeds:\n",
    "            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "            sel_idx=[]; haus_vals=[]; t0=time.time()\n",
    "\n",
    "            for c in sorted(counts):\n",
    "                idx_c   = np.where(labels==c)[0]\n",
    "                feats_c = features[idx_c]\n",
    "                n       = len(idx_c)\n",
    "                desired = max(1, math.ceil((p/100)*n))\n",
    "\n",
    "                if desired == 1:\n",
    "                    loc = np.random.choice(n, 1, replace=False)\n",
    "                    sel_idx.append(idx_c[loc].item())\n",
    "                    haus_vals.append(0.)\n",
    "                    continue\n",
    "                if desired >= n:\n",
    "                    sel_idx.extend(idx_c.tolist())\n",
    "                    haus_vals.append(0.)\n",
    "                    continue\n",
    "\n",
    "                Gc = build_knn_graph(feats_c, k_nn=k_curr)\n",
    "                A  = nx.adjacency_matrix(Gc, weight='weight').tocsr()\n",
    "                A.setdiag(1.0)\n",
    "                data,ind,ptr = convert_from_csr_matrix(A)\n",
    "                csr = csr_matrix((data,ind,ptr), shape=A.shape)\n",
    "\n",
    "                ratio = desired / n\n",
    "                model_cs = CoresetSpectralClustering(\n",
    "                    num_clusters=desired,\n",
    "                    coreset_ratio=ratio,\n",
    "                    k_over_sampling_factor=5.0,\n",
    "                    shift=0.01,\n",
    "                    full_labels=True,\n",
    "                    ignore_warnings=True\n",
    "                )\n",
    "                try:\n",
    "                    model_cs.fit(csr)\n",
    "                    core = model_cs.coreset_indices_.astype(int)\n",
    "                    if len(core) < desired:\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    loc = np.random.choice(n, desired, replace=False)\n",
    "                else:\n",
    "                    u = np.unique(core)\n",
    "                    if len(u) < desired:\n",
    "                        extra = np.random.choice(\n",
    "                            [i for i in range(n) if i not in u],\n",
    "                            desired-len(u), replace=False\n",
    "                        )\n",
    "                        loc = np.concatenate([u, extra])\n",
    "                    else:\n",
    "                        loc = u[:desired]\n",
    "\n",
    "                sel_idx.extend(idx_c[loc].tolist())\n",
    "                haus_vals.append(\n",
    "                    max(\n",
    "                        directed_hausdorff(feats_c, feats_c[loc])[0],\n",
    "                        directed_hausdorff(feats_c[loc], feats_c)[0]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if not sel_idx:\n",
    "                continue\n",
    "\n",
    "            train_ds     = NumpyDataset(\n",
    "                os.path.join(path,'train_images.npy'),\n",
    "                os.path.join(path,'train_labels.npy'),\n",
    "                transform=train_transform\n",
    "            )\n",
    "            train_loader = DataLoader(Subset(train_ds, sel_idx),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "            model = timm.create_model('vgg16', pretrained=True,\n",
    "                                      num_classes=len(counts)).to(device)\n",
    "            opt   = optim.Adam(model.parameters(), lr=lr)\n",
    "            sch   = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n",
    "                                                        factor=0.1, patience=5)\n",
    "            scaler= torch.cuda.amp.GradScaler()\n",
    "\n",
    "            logs = train_model_subset(\n",
    "                model, train_loader, val_loader,\n",
    "                device, opt, sch, scaler,\n",
    "                num_epochs=20, accumulation_steps=4,\n",
    "                patience=5, class_weights=cw\n",
    "            )\n",
    "\n",
    "            stats.append({\n",
    "                 'acc': logs['val_acc'][-1],\n",
    "                 'f1':  logs['val_f1'][-1],\n",
    "                 'haus': float(np.mean(haus_vals)),\n",
    "                 'time': time.time()-t0,\n",
    "                 'n':   len(sel_idx)\n",
    "            })\n",
    "\n",
    "        if not stats:\n",
    "            print(f\"  ε={p}% → no valid runs, skipping\")\n",
    "            continue\n",
    "\n",
    "        A = np.array([[s['acc'],s['f1'],s['haus'],s['time']] for s in stats])\n",
    "        results[p] = {\n",
    "            'acc_mean': A[:,0].mean(),\n",
    "            'acc_std':  A[:,0].std(),\n",
    "            'f1_mean':  A[:,1].mean(),\n",
    "            'haus_mean':A[:,2].mean(),\n",
    "            'time_mean':A[:,3].mean()\n",
    "        }\n",
    "        print(f\" ε={p}% → acc={results[p]['acc_mean']:.3f}±{results[p]['acc_std']:.3f}, \"\n",
    "              f\"f1={results[p]['f1_mean']:.3f}, haus={results[p]['haus_mean']:.3f}, \"\n",
    "              f\"time={results[p]['time_mean']:.1f}s\")\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
