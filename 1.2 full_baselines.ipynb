{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import timm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def hausdorff(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    d1 = directed_hausdorff(A, B)[0]\n",
    "    d2 = directed_hausdorff(B, A)[0]\n",
    "    return max(d1, d2)\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images_path: str, labels_path: str, transform=None):\n",
    "        self.images = np.load(images_path)\n",
    "        self.labels = np.load(labels_path).flatten()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.images[idx]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img] * 3, axis=0)\n",
    "        else:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        lbl = int(self.labels[idx])\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de017e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_indices(features, labels, percentages, seed):\n",
    "    feats = features.cpu().numpy()\n",
    "    labs = labels.cpu().numpy().flatten()\n",
    "    out = {}\n",
    "    for p in percentages:\n",
    "        inds = []\n",
    "        for cls in np.unique(labs):\n",
    "            idxs = np.where(labs == cls)[0]\n",
    "            k = max(1, int(len(idxs) * (p / 100)))\n",
    "            if k > 1:\n",
    "                km = KMeans(n_clusters=k, random_state=seed, n_init=10).fit(feats[idxs])\n",
    "                centers, lbls = km.cluster_centers_, km.labels_\n",
    "                for c in range(k):\n",
    "                    mem = np.where(lbls == c)[0]\n",
    "                    dists = np.linalg.norm(feats[idxs[mem]] - centers[c], axis=1)\n",
    "                    inds.append(idxs[mem[np.argmin(dists)]])\n",
    "            else:\n",
    "                centroid = feats[idxs].mean(axis=0)\n",
    "                rep = idxs[np.argmin(np.linalg.norm(feats[idxs] - centroid, axis=1))]\n",
    "                inds.append(rep)\n",
    "        while set(labs[inds]) != set(np.unique(labs)):\n",
    "            missing = set(np.unique(labs)) - set(labs[inds])\n",
    "            for cls in missing:\n",
    "                inds.append(np.random.choice(np.where(labs == cls)[0]))\n",
    "        out[p] = sorted(inds)\n",
    "    return out\n",
    "\n",
    "def propagation_indices(propagated_feats, labels, percentage):\n",
    "    pf = propagated_feats.cpu()\n",
    "    labs = labels.cpu()\n",
    "    inds = []\n",
    "    for cls in torch.unique(labs):\n",
    "        idxs = (labs == cls).nonzero(as_tuple=True)[0]\n",
    "        X = pf[idxs]\n",
    "        cent = X.mean(dim=0, keepdim=True)\n",
    "        sims = F.cosine_similarity(X, cent)\n",
    "        k = max(1, int(len(idxs) * (percentage / 100)))\n",
    "        topk = sims.topk(k).indices\n",
    "        inds.extend(idxs[topk].cpu().tolist())\n",
    "    return sorted(inds)\n",
    "\n",
    "def spectral_indices(spectral_emb, labels, percentage, seed):\n",
    "    X = spectral_emb.cpu().numpy()\n",
    "    labs = labels.cpu().numpy().flatten()\n",
    "    inds = []\n",
    "    for cls in np.unique(labs):\n",
    "        idxs = np.where(labs == cls)[0]\n",
    "        sub = X[idxs]\n",
    "        k = max(1, int(len(idxs) * (percentage / 100)))\n",
    "        if k == 1:\n",
    "            sim = cosine_similarity(sub)\n",
    "            inds.append(idxs[np.argmax(sim.mean(axis=1))])\n",
    "        else:\n",
    "            km = KMeans(n_clusters=k, random_state=seed).fit(sub)\n",
    "            for c in range(k):\n",
    "                mem = np.where(km.labels_ == c)[0]\n",
    "                simc = cosine_similarity(sub[mem])\n",
    "                inds.append(idxs[mem[np.argmax(simc.mean(axis=1))]])\n",
    "    return sorted(inds)\n",
    "\n",
    "\n",
    "def train_model_subset(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    num_epochs: int = 20,\n",
    "    accumulation_steps: int = 4,\n",
    "    patience: int = 5,\n",
    "    class_weights: torch.Tensor = None\n",
    ") -> dict:\n",
    "    metrics_log = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "        \"val_f1\": []\n",
    "    }\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device)) if class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_preds = [], []\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                try:\n",
    "                    scaler.step(optimizer)\n",
    "                except AssertionError:\n",
    "                    optimizer.step()\n",
    "                try:\n",
    "                    scaler.update()\n",
    "                except AssertionError:\n",
    "                    pass\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        if (step + 1) % accumulation_steps != 0:\n",
    "            try:\n",
    "                scaler.step(optimizer)\n",
    "            except AssertionError:\n",
    "                optimizer.step()\n",
    "            try:\n",
    "                scaler.update()\n",
    "            except AssertionError:\n",
    "                pass\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_labels, val_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "        metrics_log[\"train_loss\"].append(train_loss)\n",
    "        metrics_log[\"train_accuracy\"].append(train_accuracy)\n",
    "        metrics_log[\"train_f1\"].append(train_f1)\n",
    "        metrics_log[\"val_loss\"].append(val_loss)\n",
    "        metrics_log[\"val_accuracy\"].append(val_accuracy)\n",
    "        metrics_log[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(\"Validation loss improved. Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Final Train Loss:     {metrics_log['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Train Accuracy: {metrics_log['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final Train F1:       {metrics_log['train_f1'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss:       {metrics_log['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Accuracy:   {metrics_log['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final Val F1:         {metrics_log['val_f1'][-1]:.4f}\")\n",
    "\n",
    "    return metrics_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a254e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    DATA_PATH = '/home/dime/Desktop/Thesis/dermamnist_224'\n",
    "    percentages = [0.1, 1, 10]\n",
    "    seeds = [42, 43, 45]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_labels = np.load(os.path.join(DATA_PATH, 'train_labels.npy')).flatten()\n",
    "    class_counts = Counter(train_labels)\n",
    "    total = sum(class_counts.values())\n",
    "    weights = [total / class_counts[i] for i in range(len(class_counts))]\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    summary_records = []\n",
    "\n",
    "    for emb in ['vit_derma', 'vgg_derma']:\n",
    "        print(f\"\\n===== Experiments for {emb} embeddings =====\")\n",
    "        features = torch.load(f\"{emb}_train_features.pth\")\n",
    "        propagated = torch.load(f\"propagated_{emb}_train_features.pth\")\n",
    "        spectral = torch.load(f\"spectral_{emb}_train_embeddings.pth\")\n",
    "        labels = torch.from_numpy(train_labels).long()\n",
    "\n",
    "        for method, fn in [\n",
    "            ('kmeans', lambda p, s: kmeans_indices(features, labels, [p], s)[p]),\n",
    "            ('propagation', lambda p, s: propagation_indices(propagated, labels, p)),\n",
    "            ('spectral', lambda p, s: spectral_indices(spectral, labels, p, s)),\n",
    "        ]:\n",
    "            for p in percentages:\n",
    "                print(f\"\\n======================= SUMMARY for {method.upper()} {p}% =======================\")\n",
    "                cpu_start = time.process_time()\n",
    "                wall_start = time.time()\n",
    "                accs, f1s, hds = [], [], []\n",
    "\n",
    "                for s in seeds:\n",
    "                    set_seed(s)\n",
    "                    subs = fn(p, s)\n",
    "                    ds = NumpyDataset(\n",
    "                        os.path.join(DATA_PATH,'train_images.npy'),\n",
    "                        os.path.join(DATA_PATH,'train_labels.npy')\n",
    "                    )\n",
    "                    sub_ds = Subset(ds, subs)\n",
    "                    train_loader = DataLoader(sub_ds, batch_size=16, shuffle=True)\n",
    "                    val_loader = DataLoader(\n",
    "                        NumpyDataset(\n",
    "                            os.path.join(DATA_PATH,'val_images.npy'),\n",
    "                            os.path.join(DATA_PATH,'val_labels.npy')\n",
    "                        ), batch_size=16, shuffle=False\n",
    "                    )\n",
    "                    model = timm.create_model('vgg16', pretrained=True, num_classes=7).to(device)\n",
    "                    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "                    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "                    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "                    metrics = train_model_subset(\n",
    "                        model, train_loader, val_loader,\n",
    "                        device, optimizer, scheduler, scaler,\n",
    "                        num_epochs=20, accumulation_steps=4,\n",
    "                        patience=5, class_weights=class_weights\n",
    "                    )\n",
    "                    train_loss = metrics['train_loss'][-1]\n",
    "                    train_acc = metrics['train_accuracy'][-1]\n",
    "                    train_f1  = metrics['train_f1'][-1]\n",
    "                    val_loss   = metrics['val_loss'][-1]\n",
    "                    val_acc    = metrics['val_accuracy'][-1]\n",
    "                    val_f1     = metrics['val_f1'][-1]\n",
    "\n",
    "                    full_emb = features.cpu().numpy()\n",
    "                    sub_emb  = full_emb[subs]\n",
    "                    hd = hausdorff(full_emb, sub_emb)\n",
    "\n",
    "                    accs.append(val_acc)\n",
    "                    f1s.append(val_f1)\n",
    "                    hds.append(hd)\n",
    "\n",
    "  \n",
    "                cpu_end = time.process_time()\n",
    "                wall_end = time.time()\n",
    "\n",
    "                mean_acc, std_acc = float(np.mean(accs)), float(np.std(accs))\n",
    "                mean_f1, std_f1 = float(np.mean(f1s)), float(np.std(f1s))\n",
    "                mean_hd = float(np.mean(hds))\n",
    "                cpu_time = cpu_end - cpu_start\n",
    "                wall_time = wall_end - wall_start\n",
    "\n",
    "\n",
    "                print(f\"Validation Accuracy: mean={np.mean(accs):.4f}, std={np.std(accs):.4f}\")\n",
    "                print(f\"Validation F1:       mean={np.mean(f1s):.4f}, std={np.std(f1s):.4f}\")\n",
    "                print(f\"Mean Hausdorff:      {np.mean(hds):.4f}\")\n",
    "                print(f\"CPU times:           {cpu_end - cpu_start:.2f}s\")\n",
    "                print(f\"Wall time:           {wall_end - wall_start:.2f}s\")\n",
    "\n",
    "                summary_records.append({\n",
    "                    'embedding': emb,\n",
    "                    'method': method,\n",
    "                    'percentage': p,\n",
    "                    'mean_accuracy': mean_acc,\n",
    "                    'std_accuracy': std_acc,\n",
    "                    'mean_f1': mean_f1,\n",
    "                    'std_f1': std_f1,\n",
    "                    'mean_hausdorff': mean_hd,\n",
    "                    'cpu_time_s': cpu_time,\n",
    "                    'wall_time_s': wall_time\n",
    "                })\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(summary_records)\n",
    "    csv_path = os.path.join(DATA_PATH, 'coreset_summary2.csv')\n",
    "    json_path = os.path.join(DATA_PATH, 'coreset_summary2.json')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(summary_records, f, indent=2)\n",
    "    print(f\"Saved summary to {csv_path} and {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
